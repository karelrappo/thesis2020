Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Siliverstovs2020,
author = {Siliverstovs, Boriss and Wochner, Daniel},
doi = {10.1002/for.2715},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/wp463terminal.pdf:pdf},
issn = {0277-6693},
journal = {Journal of Forecasting},
title = {{State‐Dependent Evaluation of Predictive Ability}},
year = {2021}
}
@article{Welch2008,
abstract = {Our article comprehensively reexamines the performance of variables that have been suggested by the academic literature to be good predictors of the equity premium. We find that by and large, these models have predicted poorly both in-sample (IS) and out-of-sample (OOS) for 30 years now; these models seem unstable, as diagnosed by their out-of-sample predictions and other statistics; and these models would not have helped an investor with access only to available information to profitably time the market. {\textcopyright} The Author 2007. Published by Oxford University Press on behalf of the Society for Financial Studies. All rights reserved.},
author = {Welch, Ivo and Goyal, Amit},
doi = {10.1093/rfs/hhm014},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/SSRN-id517667.pdf:pdf},
issn = {08939454},
journal = {Review of Financial Studies},
number = {4},
pages = {1455--1508},
title = {{A comprehensive look at the empirical performance of equity premium prediction}},
volume = {21},
year = {2008}
}
@article{Stock2003,
author = {Stock, James H and Watson, Mark W},
file = {:C$\backslash$:/Users/karel.rappo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stock, Watson - 2003 - Forecasting Output and Inflation The Role of Asset Prices.pdf:pdf},
journal = {Journal of Economic Literature},
pages = {788--829},
title = {{Forecasting Output and Inflation: The Role of Asset Prices}},
volume = {XLI},
year = {2003}
}
@article{Coulombe2020a,
abstract = {We move beyond Is Machine Learning Useful for Macroeconomic Forecasting? by adding the how. The current forecasting literature has focused on matching specific variables and horizons with a particularly successful algorithm. To the contrary, we study the usefulness of the underlying features driving ML gains over standard macroeconometric methods. We distinguish four so-called features (nonlinearities, regularization, cross-validation and alternative loss function) and study their behavior in both the data-rich and data-poor environments. To do so, we design experiments that allow to identify the “treatment” effects of interest. We conclude that (i) nonlinearity is the true game changer for macroeconomic prediction, (ii) the standard factor model remains the best regularization, (iii) K-fold cross-validation is the best practice and (iv) the L2 is preferred to the ē-insensitive in-sample loss. The forecasting gains of nonlinear techniques are associated with high macroeconomic uncertainty, financial stress and housing bubble bursts. This suggests that Machine Learning is useful for macroeconomic forecasting by mostly capturing important nonlinearities that arise in the context of uncertainty and financial frictions.},
archivePrefix = {arXiv},
arxivId = {2008.12477},
author = {Coulombe, Philippe Goulet and Leroux, Maxime and Stevanovic, Dalibor and Surprenant, St{\'{e}}phane},
eprint = {2008.12477},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/How{\_}is{\_}Machine{\_}Learning{\_}Useful{\_}for{\_}Macroeconomic{\_}F.pdf:pdf},
journal = {arXiv},
keywords = {Big Data,Forecasting,Machine Learning},
title = {{How is machine learning useful for macroeconomic forecasting?}},
year = {2020}
}
@article{Gilchrist2012,
author = {Gilchrist, Simon and Zakraj{\v{s}}ek, Egon},
doi = {10.1257/aer.102.4.1692},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/aer{\_}gilchrist{\_}zakrajsek.pdf:pdf},
issn = {00028282},
journal = {American Economic Review},
number = {4},
pages = {1692--1720},
title = {{Credit spreads and business cycle fluctuations}},
volume = {102},
year = {2012}
}
@article{Black1976,
abstract = {The contract price on a forward contract stays fixed for the life of the contract, while a futures contract is rewritten every day. The value of a futures contract is zero at the start of each day. The expected change in the futures price satisfies a formula like the capital asset pricing model. If changes in the futures price are independent of the return on the market, the futures price is the expected spot price. The futures market is not unique in its ability to shift risk, since corporations can do that too. The futures market is unique in the guidance it provides for producers, distributors, and users of commodities. Using assumptions like those used in deriving the original option formula, we find formulas for the values of forward contracts and commodity options in terms of the futures price and other variables. {\textcopyright} 1976.},
author = {Black, Fischer},
doi = {10.1016/0304-405X(76)90024-6},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/black1976.pdf:pdf},
issn = {0304405X},
journal = {Journal of Financial Economics},
number = {1-2},
pages = {167--179},
title = {{The pricing of commodity contracts}},
volume = {3},
year = {1976}
}
@article{Stock2003,
author = {Stock, James H and Watson, Mark W},
doi = {10.1257/jel.41.3.788},
issn = {00220515},
journal = {Journal of Economic Literature},
number = {3},
pages = {788--829},
title = {{Forecasting output and inflation: The role of asset prices}},
volume = {41},
year = {2003}
}
@article{Diaz2006,
abstract = {Background: Selection of relevant genes for sample classification is a common task in most gene expression studies, where researchers try to identify the smallest possible set of genes that can still achieve good predictive performance (for instance, for future use with diagnostic purposes in clinical practice). Many gene selection approaches use univariate (gene-by-gene) rankings of gene relevance and arbitrary thresholds to select the number of genes, can only be applied to two-class problems, and use gene selection ranking criteria unrelated to the classification algorithm. In contrast, random forest is a classification algorithm well suited for microarray data: it shows excellent performance even when most predictive variables are noise, can be used when the number of variables is much larger than the number of observations and in problems involving more than two classes, and returns measures of variable importance. Thus, it is important to understand the performance of random forest with microarray data and its possible use for gene selection. Results: We investigate the use of random forest for classification of microarray data (including multi-class problems) and propose a new method of gene selection in classification problems based on random forest. Using simulated and nine microarray data sets we show that random forest has comparable performance to other classification methods, including DLDA, KNN, and SVM, and that the new gene selection procedure yields very small sets of genes (often smaller than alternative methods) while preserving predictive accuracy. Conclusions: Because of its performance and features, random forest and gene selection using random forest should probably become part of the "standard tool-box" of methods for class prediction and gene selection with microarray data. {\textcopyright} 2006 Diaz-Uriarte and Alvarez de Andres, licensee BioMed Central Ltd.},
author = {D{\'{i}}az-Uriarte, Ram{\'{o}}n and {Alvarez de Andr{\'{e}}s}, Sara},
doi = {10.1186/1471-2105-7-3},
issn = {14712105},
journal = {BMC Bioinformatics},
pages = {1--13},
pmid = {16398926},
title = {{Gene selection and classification of microarray data using random forest}},
volume = {7},
year = {2006}
}
@article{Wright2006,
abstract = {The slope of the Treasury yield curve has often been cited as a leading economic indicator, with inversion of the curve being thought of as a harbinger of a recession. In this paper, I consider a number of probit models using the yield curve to forecast recessions. Models that use both the level of the federal funds rate and the term spread give better in-sample fit, and better out-of-sample predictive performance, than models with the term spread alone. There is some evidence that controlling for a term premium proxy as well may also help. I discuss the implications of the current shape of the yield curve in the light of these results, and report results of some tests for structural stability and an evaluation of out-of-sample predictive performance.},
author = {Wright, Jonathan H.},
doi = {10.17016/feds.2006.07},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/feds.pdf:pdf},
issn = {19362854},
journal = {Finance and Economics Discussion Series},
number = {07},
pages = {1--19},
title = {{The Yield Curve and Predicting Recessions}},
volume = {2006},
year = {2006}
}
@book{Ederington1993,
abstract = {We examine the impact of scheduled macroeconomic news announcements on interest rate and foreign exchange futures markets. We find these announcements are responsible for most of the observed time‐of‐day and day‐of‐the‐week volatility patterns in these markets. While the bulk of the price adjustment to a major announcement occurs within the first minute, volatility remains substantially higher than normal for roughly fifteen minutes and slightly elevated for several hours. Nonetheless, these subsequent price adjustments are basically independent of the first minute's return. We identify those announcements with the greatest impact on these markets. 1993 The American Finance Association},
author = {Ederington, LOUIS H. and Lee, JAE HA},
booktitle = {The Journal of Finance},
doi = {10.1111/j.1540-6261.1993.tb04750.x},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/ederington1993.pdf:pdf},
isbn = {1111111111},
issn = {15406261},
number = {4},
pages = {1161--1191},
title = {{How Markets Process Information: News Releases and Volatility}},
volume = {48},
year = {1993}
}
@article{Ewing2005,
abstract = {Most studies that have examined the relationship between the housing market and the macroeconomy have focused on how changes in housing supply affect real activity and the like. In this paper, the possibility that housing starts respond to sudden changes or shocks to macroeconomic factors is explicitly accounted for. The empirical methodology employs the recently developed technique of generalized impulse response analysis Pesaran and Shin (1998). The results highlight the endogeneity that exists among the housing market and macroeconomic activity. {\textcopyright} 2005 Taylor and Francis Group Ltd.},
author = {Ewing, Bradley T. and Wang, Yongsheng},
doi = {10.1080/1350485052000337806},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/single housing starts.pdf:pdf},
issn = {13504851},
journal = {Applied Economics Letters},
number = {3},
pages = {187--190},
title = {{Single housing starts and macroeconomic activity: An application of generalized impulse response analysis}},
volume = {12},
year = {2005}
}
@article{Burda2017,
abstract = {Background: Systematic reviewers often encounter incomplete or missing data, and the information desired may be difficult to obtain from a study author. Thus, systematic reviewers may have to resort to estimating data from figures with little or no raw data in a study's corresponding text or tables. Methods: We discuss a case study in which participants used a publically available Web-based program, called webplotdigitizer, to estimate data from 2 figures. We evaluated and used the intraclass coefficient and the accuracy of the estimates to the true data to inform considerations when using estimated data from figures in systematic reviews. Results: The estimates for both figures were consistent, although the distribution of estimates in the figure of a continuous outcome was slightly higher. For the continuous outcome, the percent difference ranged from 0.23{\%} to 30.35{\%} while the percent difference of the event rate ranged from 0.22{\%} to 8.92{\%}. For both figures, the intraclass coefficient was excellent ({\textgreater}0.95). Conclusions: Systematic reviewers should consider and be transparent when estimating data from figures when the information cannot be obtained from study authors and perform sensitivity analyses of pooled results to reduce bias.},
author = {Burda, Brittany U. and O'Connor, Elizabeth A. and Webber, Elizabeth M. and Redmond, Nadia and Perdue, Leslie A.},
doi = {10.1002/jrsm.1232},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/burda2017.pdf:pdf},
issn = {17592887},
journal = {Research Synthesis Methods},
keywords = {data extraction,meta-analysis,reporting bias,systematic review},
number = {3},
pages = {258--262},
pmid = {28268241},
title = {{Estimating data from figures with a Web-based program: Considerations for a systematic review}},
volume = {8},
year = {2017}
}
@article{Coulombe2020a,
abstract = {We move beyond Is Machine Learning Useful for Macroeconomic Forecasting? by adding the how. The current forecasting literature has focused on matching specific variables and horizons with a particularly successful algorithm. To the contrary, we study the usefulness of the underlying features driving ML gains over standard macroeconometric methods. We distinguish four so-called features (nonlinearities, regularization, cross-validation and alternative loss function) and study their behavior in both the data-rich and data-poor environments. To do so, we design experiments that allow to identify the “treatment” effects of interest. We conclude that (i) nonlinearity is the true game changer for macroeconomic prediction, (ii) the standard factor model remains the best regularization, (iii) K-fold cross-validation is the best practice and (iv) the L2 is preferred to the ē-insensitive in-sample loss. The forecasting gains of nonlinear techniques are associated with high macroeconomic uncertainty, financial stress and housing bubble bursts. This suggests that Machine Learning is useful for macroeconomic forecasting by mostly capturing important nonlinearities that arise in the context of uncertainty and financial frictions.},
archivePrefix = {arXiv},
arxivId = {2008.12477},
author = {Coulombe, Philippe Goulet and Leroux, Maxime and Stevanovic, Dalibor and Surprenant, St{\'{e}}phane},
eprint = {2008.12477},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/How{\_}is{\_}Machine{\_}Learning{\_}Useful{\_}for{\_}Macroeconomic{\_}F.pdf:pdf},
journal = {arXiv},
keywords = {Big Data,Forecasting,Machine Learning},
title = {{How is machine learning useful for macroeconomic forecasting?}},
year = {2020}
}
@article{David2014,
abstract = {Shocks to equity options' implied volatility are followed by persistently lower short-term rates. Shocks to puts' over calls' out-of-the-money implied volatilities (P/C) are followed by persistently higher rates. Stock and Treasury bond implied volatilities, which measure market and policy uncertainty, are countercyclical, while P/C, which measures downside risk, is procyclical. An equilibrium model in which investors and the central bank learn about composite regimes of economic and policy variables explains these dynamics, linking them to a learning-based, forward-looking Taylor rule. Survey data support our model's predictions on the effect of uncertainty on the level and fluctuations of implied volatilities. {\textcopyright} The Author 2014.},
author = {David, Alexander and Veronesi, Pietro},
doi = {10.1093/rfs/hhu024},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/baka/Lit review/david2014.pdf:pdf},
issn = {14657368},
journal = {Review of Financial Studies},
number = {6},
pages = {1661--1716},
title = {{Investors' and Central Bank's uncertainty embedded in index options}},
volume = {27},
year = {2014}
}
@unpublished{Siliverstovs2021,
author = {Siliverstovs, Boriss},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/NCSTEAUPDATEV02.pdf:pdf},
keywords = {2nd vienna workshop on,and constructive comments on,c22,c32,c53,covid-19,e37,earlier version of,euro area gdp,forecasting,jel codes,kholodilin for his helpful,nowcasts,outliers,part of the material,the author also thanks,the author is grateful,this paper,to konstantin a,was presented at the},
title = {{Gauging the Effect of Influential Observations on Measures of Relative Forecast Accuracy in a Post-COVID-19 Era: Application to Nowcasting Euro Area GDP Growth}},
year = {2021}
}
@unpublished{Gurkaynak2011,
abstract = {The discount function, which determines the value of all future nominal payments, is the most basic building block of finance and is usually inferred from the Treasury yield curve. It is therefore surprising that researchers and practitioners do not have available to them a long history of high-frequency yield curve estimates. This paper fills that void by making public the Treasury yield curve estimates of the Federal Reserve Board at a daily frequency from 1961 to the present. We use a well-known and simple smoothing method that is shown to fit the data very well. The resulting estimates can be used to compute yields or forward rates for any horizon. We hope that the data, which are posted on the website, and which will be updated periodically, will provide a benchmark yield curve that will be useful to applied economists.},
author = {Gurkaynak, Refet S. and Sack, Brian P. and Wright, Jonathan H.},
booktitle = {Finance and Economics Discussion Series},
doi = {10.2139/ssrn.920183},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/200628pap.pdf:pdf},
title = {{The U.S. Treasury Yield Curve: 1961 to the Present}},
url = {https://www.federalreserve.gov/pubs/feds/2006/200628/200628abs.html},
year = {2006}
}
@misc{Alfred2020,
author = {{Archival FRED}},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Thesis{\_}Project{\_}files/Dataset.csv:csv},
title = {{Dataset. (Vintage 2020-20-12)}},
url = {https://alfred.stlouisfed.org/},
year = {2020}
}
@article{Cremers2017,
abstract = {We show that the level of at-the-money implied volatility from the Treasury derivatives market (Treasury ‘yield implied volatility') predicts both the level and volatility of macroeconomic activity such as the growth rates of GDP, industrial production, consumption, and employment, as well as of financial variables such as the level of interest rates and the slope of the term structure, the Libor-OIS spread, and bank credit. This predictability is robust to controlling for the short-term interest rate and the term spread, stock returns and stock market implied volatility. Treasury yield implied volatility thus constitutes a useful forward-looking state variable to characterize risks and opportunities in the macroeconomy.},
author = {Cremers, Martijn and Fleckenstein, Matthias and Gandhi, Priyank},
doi = {10.2139/ssrn.3006473},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/SSRN-id3006473.pdf:pdf},
issn = {1556-5068},
journal = {Journal of Financial Economics (JFE), Forthcoming},
keywords = {business,corresponding author,cremers,cycles,edu,email,fleckenstein,forecasting,gandhi,implied volatility,interest rate,macroeconomic activity,macroeconomic uncertainty,mcremers,mendoza college of business,mflecken,nd,options on treasury futures,real activity,rutgers business school,rutgers university,treasury futures,udel,university of delaware,university of notre dame},
title = {{Treasury Yield Implied Volatility and Real Activity}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=3006473{\#}},
year = {2017}
}
@misc{Niculaescu2018,
author = {Niculaescu, Oana},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Project{\_}Files/images/trees.png:png},
title = {{Classifying data with decision trees}},
url = {https://elf11.github.io/2018/07/01/python-decision-trees-acm.html?fbclid=IwAR0Ddeyycvj1Ao-BFIRgSy2yh0V-vmwA6THm4kVFQaXgcWhHLFp9mWeWUM0},
year = {2018}
}
@article{Cremers2021,
abstract = {We show that at-the-money implied volatility of options on futures of five-year Treasury notes (Treasury “yield implied volatility”) predicts both the growth rate and volatility of gross domestic product, as well as of other macroeconomic variables, like industrial production, consumption, and employment. This predictability is robust to controlling for the term spread, credit spread, stock returns, stock market implied volatility, and several other variables that prior literature showed to predict macroeconomic activity. Our results indicate that Treasury yield implied volatility is a useful forward-looking state variable to characterize risks and opportunities in the macro economy.},
author = {Cremers, Martijn and Fleckenstein, Matthias and Gandhi, Priyank},
doi = {10.1016/j.jfineco.2020.12.009},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/YIv NEWEST.pdf:pdf},
issn = {0304405X},
journal = {Journal of Financial Economics},
keywords = {Forecasting,Implied volatility,Macroeconomic activity,Macroeconomic uncertainty,Treasury futures and options},
number = {xxxx},
publisher = {Elsevier B.V.},
title = {{Treasury yield implied volatility and real activity}},
url = {https://doi.org/10.1016/j.jfineco.2020.12.009},
year = {2021}
}
@misc{GZdata,
address = {Nashville, TN},
author = {Gilchrist, Simon and Zakraj{\v{s}}ek, Egon},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/thesis2020/Data/GZ{\_}quarterly.csv:csv},
publisher = {American Economic Association},
title = {{Replication data for: Credit Spreads and Business Cycle Fluctuations.}},
url = {https://www.openicpsr.org/openicpsr/project/112536/version/V1/view},
year = {2019}
}
@article{Ang2006,
abstract = {A lot, including a few things you may not expect. Previous studies find that the term spread forecasts GDP but these regressions are unconstrained and do not model regressor endogeneity. We build a dynamic model for GDP growth and yields that completely characterizes expectations of GDP. The model does not permit arbitrage. Contrary to previous findings, we predict that the short rate has more predictive power than any term spread. We confirm this finding by forecasting GDP out-of-sample. The model also recommends the use of lagged GDP and the longest maturity yield to measure slope. Greater efficiency enables the yield-curve model to produce superior out-of-sample GDP forecasts than unconstrained OLS regressions at all horizons. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Ang, Andrew and Piazzesi, Monika and Wei, Min},
doi = {10.1016/j.jeconom.2005.01.032},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/APW.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Financial markets and the macroeconomy,Forecasting,Monetary policy,Term structure},
number = {1-2},
pages = {359--403},
title = {{What does the yield curve tell us about GDP growth?}},
volume = {131},
year = {2006}
}
@article{Breiman2001,
abstract = {The role of probabilistic methods in discrete mathematics cannot be overestimated. By defining the probability measure on a set of the studied combinatorial objects various numerical characteristics of these objects can be considered as random variables and studied using the methods of probability theory. The advantage of this approach is the well-developed probabilistic analytic techniques that allow us in many cases to obtain results, the proof of which by other methods appears too complicated, if indeed it is at all possible. The application of probabilistic methods is connected with extensive use of the terminology of probability theory. The reader will easily understand however that one speaks in fact about solving enumerative problems of discrete analysis. One of the primary research lines is the study of the limit properties of combinatorial objects manifested at the unlimited increase of the number of elements comprising such objects. It is often possible to represent the distributions of the characteristics of combinatorial objects as conditional distributions of the sums of independent random variables so that they can be studied using asymptotic methods in probability theory, namely limit theorems for sums of independent random variables.},
author = {Breiman, Leo},
doi = {10.1201/9780429469275-8},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/Breiman2001{\_}Article{\_}RandomForests.pdf:pdf},
isbn = {9783110941975},
journal = {Machine Learning},
keywords = {classification,ensemble,regression},
pages = {5--32},
title = {{Random forests}},
volume = {45},
year = {2001}
}
@article{Verikas2016,
abstract = {This study analyzes muscle activity, recorded in an eight-channel electromyographic (EMG) signal stream, during the golf swing using a 7-iron club and exploits information extracted from EMG dynamics to predict the success of the resulting shot. Muscles of the arm and shoulder on both the left and right sides, namely flexor carpi radialis, extensor digitorum communis, rhomboideus and trapezius, are considered for 15 golf players (∼5 shots each). The method using Gaussian filtering is outlined for EMG onset time estimation in each channel and activation sequence profiling. Shots of each player revealed a persistent pattern of muscle activation. Profiles were plotted and insights with respect to player effectiveness were provided. Inspection of EMG dynamics revealed a pair of highest peaks in each channel as the hallmark of golf swing, and a custom application of peak detection for automatic extraction of swing segment was introduced. Various EMG features, encompassing 22 feature sets, were constructed. Feature sets were used individually and also in decision-level fusion for the prediction of shot effectiveness. The prediction of the target attribute, such as club head speed or ball carry distance, was investigated using random forest as the learner in detection and regression tasks. Detection evaluates the personal effectiveness of a shot with respect to the player-specific average, whereas regression estimates the value of target attribute, using EMG features as predictors. Fusion after decision optimization provided the best results: the equal error rate in detection was 24.3{\%} for the speed and 31.7{\%} for the distance; the mean absolute percentage error in regression was 3.2{\%} for the speed and 6.4{\%} for the distance. Proposed EMG feature sets were found to be useful, especially when used in combination. Rankings of feature sets indicated statistics for muscle activity in both the left and right body sides, correlation-based analysis of EMG dynamics and features derived from the properties of two highest peaks as important predictors of personal shot effectiveness. Activation sequence profiles helped in analyzing muscle orchestration during golf shot, exposing a specific avalanche pattern, but data from more players are needed for stronger conclusions. Results demonstrate that information arising from an EMG signal stream is useful for predicting golf shot success, in terms of club head speed and ball carry distance, with acceptable accuracy. Surface EMG data, collected with a goal to automatically evaluate golf player's performance, enables wearable computing in the field of ambient intelligence and has potential to enhance exercising of a long carry distance drive.},
author = {Verikas, Antanas and Vaiciukynas, Evaldas and Gelzinis, Adas and Parker, James and {Charlotte Olsson}, M.},
doi = {10.3390/s16040592},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/verikas2016.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Decision fusion,EMG,Muscle activity onset,Peak detection,Random forest},
number = {4},
pmid = {27120604},
title = {{Electromyographic patterns during golf swing: Activation sequence profiling and prediction of shot effectiveness}},
volume = {16},
year = {2016}
}
@article{Fornari2019,
abstract = {Published in 1999. The issue of financial volatility, especially since financial deregulation, has given rise to concerns regarding the effects of increased financial volatility on real economic activity. Two issues represent a substantial challenge to financial economists with respect to these concerns. The first relates to the identification of the causes of increased volatility in financial markets. Identification is a first step towards increasing both financial economists' and policy-makers' understanding of the interrelated causes of financial volatility. The second requires linking the effects of increased financial volatility to the real sector of the economy by examining the channels through which financial volatility influences fundamental economic variables. In order to address these two issues, the analysis initially develops and estimates a model which is capable of explaining the financial and business cycle determinates of movements in the conditional volatility of the Australian All Industrials stock market index. Evidence suggests that a significant linkage exists between the conditional volatility of the money supply. Models are then developed to examine how monetary volatility is transmitted to the volatility of financial asset prices, inflation and real output in an open economy. The results indicate that while financial volatility has increased to some extent since the late 1980s, this has been transferred non-uniformly towards increasing volatility of both real and financial activity.},
author = {Fornari, Fabio and Mele, Antonio},
doi = {10.4324/9780429456572},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/baka/2282-717X-03000-3.pdf:pdf},
isbn = {9780429852145},
journal = {Journal of Financial Management},
number = {2},
pages = {155--196},
title = {{Financial volatility and real economic activity}},
volume = {1},
year = {2019}
}
@misc{vix,
author = {CBOE},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/thesis2020/Data/vixarchive.csv:csv},
title = {{Vix Dataset}},
url = {https://ww2.cboe.com/products/vix-index-volatility/vix-options-and-futures/vix-index/vix-historical-data},
year = {2020}
}
@phdthesis{Coulombe2020,
abstract = {Over the last decades, an impressive amount of non-linearities have been proposed to reconcile reduced-form macroeconomic models with the data. Many of them boil down to have linear regression coefficients evolving through time: threshold/switching/smooth-transition regression; structural breaks and random walk time-varying parameters. While all of these schemes are reasonably plausible in isolation, I argue that those are much more in agreement with the data if they are combined. To this end, I propose Macroeconomic Random Forests, which adapts the canonical Machine Learning (ML) algorithm to the problem of flexibly modeling evolving parameters in a linear macro equation. The approach exhibits clear forecasting gains over a wide range of alternatives and successfully predicts the drastic 2008 rise in unemployment. The obtained generalized time-varying parameters (GTVPs) are shown to behave differently compared to random walk coefficients by adapting nicely to the problem at hand, whether it is regime-switching behavior or long-run structural change. By dividing the typical ML interpretation burden into looking at each TVP separately, I find that the resulting forecasts are, in fact, quite interpretable. An application to the US Phillips curve reveals it is probably not flattening the way you think.},
archivePrefix = {arXiv},
arxivId = {2006.12724},
author = {Coulombe, Philippe Goulet},
booktitle = {arXiv},
doi = {10.2139/ssrn.3633110},
eprint = {2006.12724},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/baka/Lit review/SSRN-id3633110.pdf:pdf},
issn = {1556-5068},
title = {{The Macroeconomy as a Random Forest}},
year = {2020}
}
@article{Brandt2007,
abstract = {The article discusses the study "Bridging the Work/Social Divide: The Emotional Response to Organizational Social Networking Sites," published in the "European Journal of Information Systems." The study found that the participation of employees in a work-sponsored internal social networking site (SNS) improved morale and reduced turnover. Ways the SNS helped the new hires build social capital are discussed. Also included are suggestions for organizations before implementing SNSs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.6943v1},
author = {Brandt, Michael W. and Kavajecz, Kenneth A. and Underwood, Shane E.},
doi = {10.1002/fut},
eprint = {arXiv:1406.6943v1},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/cti.pdf:pdf},
isbn = {9789078328810},
issn = {0160-0176},
journal = {The Journal of Futures Markets},
number = {11},
pages = {1021--1051},
title = {{Price discovery in the treasury futures market}},
volume = {27},
year = {2007}
}
@article{Ferrara2014,
abstract = {The Great Recession endured by the main industrialized countries during the period 2008-2009, in the wake of the financial and banking crisis, has pointed out the major role of the financial sector on macroeconomic fluctuations. In this respect, many researchers have started to reconsider the linkages between financial and macroeconomic areas. In this paper, we evaluate the leading role of the daily volatility of two major financial variables, namely commodity and stock prices, in their ability to anticipate the output growth. For this purpose, we propose an extended MIDAS model that allows the forecasting of the quarterly output growth rate using exogenous variables sampled at various higher frequencies. Empirical results on three industrialized countries (US, France, and UK) show that mixing daily financial volatilities and monthly industrial production is useful at the time of predicting gross domestic product growth over the Great Recession period. {\textcopyright} 2013 Elsevier B.V.},
author = {Ferrara, Laurent and Marsilli, Cl{\'{e}}ment and Ortega, Juan Pablo},
doi = {10.1016/j.econmod.2013.08.042},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/baka/ferrara2014.pdf:pdf},
issn = {02649993},
journal = {Economic Modelling},
keywords = {Financial variables,Forecasting,Great Recession,MIDAS approach,Volatility},
pages = {44--50},
publisher = {Elsevier B.V.},
title = {{Forecasting growth during the Great Recession: Is financial volatility the missing ingredient?}},
url = {http://dx.doi.org/10.1016/j.econmod.2013.08.042},
volume = {36},
year = {2014}
}
@article{Chauvet2013,
abstract = {This chapter surveys the recent literature on output forecasting, and examines the real-time forecasting ability of several models for U.S. output growth. In particular, it evaluates the accuracy of short-term forecasts of linear and nonlinear structural and reduced-form models, and judgmental forecasts of output growth. Our emphasis is on using solely the information that was available at the time the forecast was being made, in order to reproduce the forecasting problem facing forecasters in real-time. We find that there is a large difference in forecast performance across business cycle phases. In particular, it is much harder to forecast output growth during recessions than during expansions. Simple linear and nonlinear autoregressive models have the best accuracy in forecasting output growth during expansions, although the dynamic stochastic general equilibrium model and the vector autoregressive model with financial variables do relatively well. On the other hand, we find that most models do poorly in forecasting output growth during recessions. The autoregressive model based on the nonlinear dynamic factor model that takes into account asymmetries between expansions and recessions displays the best real time forecast accuracy during recessions. Even though the Blue Chip forecasts are comparable, the dynamic factor Markov switching model has better accuracy, particularly with respect to the timing and depth of output fall during recessions in real time. The results suggest that there are large gains in considering separate forecasting models for normal times and models especially designed for periods of abrupt changes, such as during recessions and financial crises. {\textcopyright} 2013 Elsevier B.V.},
author = {Chauvet, Marcelle and Potter, Simon},
doi = {10.1016/B978-0-444-53683-9.00003-7},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/baka/chauvet2013.pdf:pdf},
isbn = {9780444536839},
issn = {15740706},
journal = {Handbook of Economic Forecasting},
keywords = {DSGE models,Dynamic factor model,Evaluating forecasts,Macroeconomic forecasting,Markov switching,Nonlinear,Real time,Recession,Vector autoregressive model},
pages = {141--194},
publisher = {Elsevier B.V.},
title = {{Forecasting output}},
url = {http://dx.doi.org/10.1016/B978-0-444-53683-9.00003-7},
volume = {2},
year = {2013}
}
@book{Stock1989,
abstract = {The system of Leading and Coincident Economic Indicators, currently maintained by the U.S. Department of Commerce (DOC), was developed as part of the NBER research program on business cycles over fifty years ago. This paper uses recent developments in econometric methodology and computing technology to take a fresh look at this system. The result is three experimental indexes. The first, constructed using a dynamic factor model, is numerically similar to the current index of coincident indicators maintained by the DOC. The second, an alternative index of leading indicators, is designed to forecast the growth in the DOC index over a six month horizon. The third-a "Recession Index"-estimates the probability that the economy will be in a recession six months hence. Only two of the seven series in the proposed leading index are used by the DOC to construct their index. Of these new series, interest rates (a public-private risk premium and the slope of the yield curve) are found to be particularly useful predictors of future economic activity.},
author = {Stock, J. H. and Watson, M. W.},
booktitle = {NBER Macroeconomics Annual},
doi = {10.2307/3584985},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/New Indexes of Coincident and Leading Economic Indicators.pdf:pdf},
isbn = {0262022966},
issn = {08893365},
pages = {351},
title = {{New Indexes of Coincident and Leading Economic Indicators}},
volume = {4},
year = {1989}
}
@article{Ederington1996,
author = {Ederington, LH and Lee, JH},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/ederington1996.pdf:pdf},
journal = {The Journal of Financial and Quantitative Analysis},
number = {4},
pages = {513--539},
title = {{The Creation and Resolution of Market Uncertainty: The Impact of Information Releases on Implied Volatility}},
url = {http://journals.cambridge.org/abstract{\_}S0022109000023784},
volume = {31},
year = {1996}
}
@unpublished{Bolhuis2020,
author = {Bolhuis, Marijn A and Rayner, Brett},
institution = {IMF},
keywords = {Cross-validation,Ensemble,Forecasts,GDP growth,Machine learning,Nowcasting,Random Forest,Turkey.},
title = {{Deus ex Machina? A Framework for Macro Forecasting with Machine Learning}},
year = {2020}
}
@article{Dai2007,
abstract = {This article develops and empirically implements an arbitrage-free, dynamic term structure model with priced factor and regime-shift risks. The risk factors are assumed to follow a discrete-time Gaussian process, and regime shifts are governed by a discrete-time Markov process with state-dependent transition probabilities. This model gives closed-form solutions for zero-coupon bond prices, an analytic representation of the likelihood function for bond yields, and a natural decomposition of expected excess returns to components corresponding to regime-shift and factor risks. Using monthly data on U.S. Treasury zero-coupon bond yields, we show a critical role of priced, state-dependent regime-shift risks in capturing the time variations in expected excess returns, and document notable differences in the behaviors of the factor risk component of the expected returns across high and low volatility regimes. Additionally, the state dependence of the regime-switching probabilities is shown to capture an interesting asymmetry in the cyclical behavior of interest rates. The shapes of the term structure of volatility of bond yield changes are also very different across regimes, with the well-known hump being largely a low-volatility regime phenomenon. {\textcopyright} TheAuthor 2007. Published byOxfordUniversity Press on behalf of The Society for Financial Studies. All rights reserved.},
author = {Dai, Qiang and Singleton, Kenneth J. and Yang, Wei},
doi = {10.1093/rfs/hhm021},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/dai2007.pdf:pdf},
issn = {08939454},
journal = {Review of Financial Studies},
number = {5},
pages = {1669--1706},
title = {{Regime shifts in a dynamic term structure model of U.S. treasury bond yields}},
volume = {20},
year = {2007}
}
@article{Carrasco2016,
abstract = {This article considers in-sample prediction and out-of-sample forecasting in regressions with many exogenous predictors. We consider four dimension-reduction devices: principal components, ridge, Landweber Fridman, and partial least squares. We derive rates of convergence for two representative models: an ill-posed model and an approximate factor model. The theory is developed for a large cross-section and a large time-series. As all these methods depend on a tuning parameter to be selected, we also propose data-driven selection methods based on cross-validation and establish their optimality. Monte Carlo simulations and an empirical application to forecasting inflation and output growth in the U.S. show that data-reduction methods outperform conventional methods in several relevant settings, and might effectively guard against instabilities in predictors' forecasting ability.},
author = {Carrasco, Marine and Rossi, Barbara},
doi = {10.1080/07350015.2016.1186029},
issn = {15372707},
journal = {Journal of Business and Economic Statistics},
keywords = {Factor models,Forecasting,GDP forecasts,Inflation forecasts,Large datasets,Partial least squares,Principal components,Regularization methods,Ridge,Sparsity,Variable selection},
number = {3},
pages = {313--338},
title = {{In-Sample Inference and Forecasting in Misspecified Factor Models}},
volume = {34},
year = {2016}
}
@misc{spy,
author = {NASDAQ},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/thesis2020/Data/SPY.csv:csv},
title = {{SPDR S{\&}P 500 ETF Trust (SPY)}},
url = {https://finance.yahoo.com/quote/SPY/history/},
year = {2020}
}
@article{Zhang2012,
abstract = {It is well known that random forests reduce the variance of the regression predictors compared to a single tree, while leaving the bias unchanged. In many situations, the dominating component in the risk turns out to be the squared bias, which leads to the necessity of bias correction. In this paper, random forests are used to estimate the regression function. Five different methods for estimating bias are proposed and discussed. Simulated and real data are used to study the performance of these methods. Our proposed methods are significantly effective in reducing bias in regression context. {\textcopyright} 2012 Copyright Taylor and Francis Group, LLC.},
author = {Zhang, Guoyi and Lu, Yan},
doi = {10.1080/02664763.2011.578621},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/biasrf.pdf:pdf},
issn = {02664763},
journal = {Journal of Applied Statistics},
keywords = {bias correction,mean-squared prediction error,random forests,regression,simulation},
number = {1},
pages = {151--160},
title = {{Bias-corrected random forests in regression}},
volume = {39},
year = {2012}
}
@article{Woloszko2020,
author = {Woloszko, Nicolas},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/ECO-WKP(2020)1.pdf:pdf},
number = {Working papers no. 1593},
pages = {1--43},
title = {{Adaptive Trees : A New Approach To Economic Forecasting}},
year = {2020}
}
@book{Stock2016,
abstract = {This chapter provides an overview of and user's guide to dynamic factor models (DFMs), their estimation, and their uses in empirical macroeconomics. It also surveys recent developments in methods for identifying and estimating SVARs, an area that has seen important developments over the past 15 years. The chapter begins by introducing DFMs and the associated statistical tools, both parametric (state-space forms) and nonparametric (principal components and related methods). After reviewing two mature applications of DFMs, forecasting and macroeconomic monitoring, the chapter lays out the use of DFMs for analysis of structural shocks, a special case of which is factor-augmented vector autoregressions (FAVARs). A main focus of the chapter is how to extend methods for identifying shocks in structural vector autoregression (SVAR) to structural DFMs. The chapter provides a unification of SVARs, FAVARs, and structural DFMs and shows both in theory and through an empirical application to oil shocks how the same identification strategies can be applied to each type of model.},
author = {Stock, J. H. and Watson, M. W.},
booktitle = {Handbook of Macroeconomics},
doi = {10.1016/bs.hesmac.2016.04.002},
edition = {1},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/Stock{\_}Watson{\_}HOM{\_}Vol2.pdf:pdf},
isbn = {9780444594877},
issn = {15740048},
keywords = {Factor-augmented vector autoregressions,Large-model forecasting,Nowcasting,Principal components,State-space models,Structural shocks,Structural vector autoregressions},
pages = {415--525},
publisher = {Elsevier B.V.},
title = {{Dynamic Factor Models, Factor-Augmented Vector Autoregressions, and Structural Vector Autoregressions in Macroeconomics}},
url = {http://dx.doi.org/10.1016/bs.hesmac.2016.04.002},
volume = {2},
year = {2016}
}
@article{Drevon2017,
abstract = {Quantitative synthesis of data from single-case designs (SCDs) is becoming increasingly common in psychology and education journals. Because researchers do not ordinarily report numerical data in addition to graphical displays, reliance on plot digitizing tools is often a necessary component of this research. Intercoder reliability of data extraction is a commonly overlooked, but potentially important, step of this process. The purpose of this study was to examine the intercoder reliability and validity of WebPlotDigitizer (Rohatgi, 2015), a web-based plot digitizing tool for extracting data from a variety of plots, including XY coordinates of interrupted time-series data. Two coders extracted 3,596 data points from 168 data series in 36 graphs across 18 studies. Results indicated high levels of intercoder reliability and validity. Implications of and recommendations based on these results are discussed in relation to researchers involved in quantitative synthesis of data from SCDs.},
author = {Drevon, Daniel and Fursa, Sophie R. and Malcolm, Allura L.},
doi = {10.1177/0145445516673998},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/drevon2016.pdf:pdf},
issn = {15524167},
journal = {Behavior Modification},
keywords = {data extraction,meta-analysis,reliability,single-case design,validity},
number = {2},
pages = {323--339},
pmid = {27760807},
title = {{Intercoder Reliability and Validity of WebPlotDigitizer in Extracting Graphed Data}},
volume = {41},
year = {2017}
}
@article{Cesa-Bianchi2020,
abstract = {We develop an asset pricing model with heterogeneous exposure to a persistent world growth factor to identify global growth and financial shocks in a multicountry panel VAR in volatility and output growth. The econometric estimates yield three sets of empirical results about (1) the importance of global growth for the interpretation of the correlation between volatility and growth over the business cycle and the possible presence of omitted variable bias in single-country VAR studies, (2) the extent to which output shocks drive volatility, and (3) the transmission of volatility shocks to output growth.},
author = {Cesa-Bianchi, Ambrogio and Pesaran, M Hashem and Rebucci, Alessandro and Chudik, Alex and Diebold, Frank and Elenev, Vadim and Giannone, Domenico and Fusari, Nicola and Lenza, Michele and Noual, Pierre and Primiceri, Giorgio and Rossi, Barbara and Smith, Ron and Song, Zhaogang and Timmermann, Allan and Zaffaroni, Paolo},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/vix-gdp.pdf:pdf},
journal = {The Review of Financial Studies},
keywords = {Asset Pricing,Economic Fluctuations and Growth,International Finance and Macroeconomics},
number = {8},
pages = {3393--3445},
title = {{Uncertainty and Economic Activity: A Multicountry Perspective}},
url = {http://www.nber.org/papers/w24325},
volume = {33},
year = {2020}
}
@article{Zhang2019,
abstract = {This paper studies the interplay of machine learning and sampling scheme in an empirical analysis of money laundering detection algorithms. Using actual transaction data provided by a U.S. financial institution, we study five major machine learning algorithms including Bayes logistic regression, decision tree, random forest, support vector machine, and artificial neural network. As the incidence of money laundering events is rare, we apply and compare two sampling techniques that increase the relative presence of the events. Our analysis reveals potential advantages of machine learning algorithms in modeling money laundering events. This paper provides insights into the use of machine learning and sampling schemes in money laundering detection specifically, and classification of rare events in general.},
author = {Zhang, Yan and Trubey, Peter},
doi = {10.1007/s10614-018-9864-z},
file = {:C$\backslash$:/Users/karel.rappo/Desktop/Bachelor Thesis 2020/Lit review/2-Machine-Learning-and-Sampling-Scheme.pdf:pdf},
isbn = {1061401898},
issn = {15729974},
journal = {Computational Economics},
keywords = {Bootstrap,Machine learning,Money laundering,Rare event,Sampling scheme},
number = {3},
pages = {1043--1063},
publisher = {Springer US},
title = {{Machine Learning and Sampling Scheme: An Empirical Study of Money Laundering Detection}},
url = {https://doi.org/10.1007/s10614-018-9864-z},
volume = {54},
year = {2019}
}
